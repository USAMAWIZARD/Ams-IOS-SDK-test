From d1f5f2e48bbd3177d8c6e7e801f34c9ae4531262 Mon Sep 17 00:00:00 2001
From: mekya <ahmetmermerkaya@gmail.com>
Date: Sat, 26 Nov 2022 15:44:59 +0300
Subject: [PATCH] Patch RTCAudioDeviceModule and Mic Permission for Playback

---
 sdk/BUILD.gn                                  |   7 +
 .../RTCAudioDeviceModule+Private.h            |  16 ++
 .../api/peerconnection/RTCAudioDeviceModule.h |  18 +++
 .../peerconnection/RTCAudioDeviceModule.mm    |  27 ++++
 .../peerconnection/RTCPeerConnectionFactory.h |   5 +
 .../RTCPeerConnectionFactory.mm               |  27 ++++
 sdk/objc/native/src/audio/audio_device_ios.h  |  10 +-
 sdk/objc/native/src/audio/audio_device_ios.mm | 145 +++++++++++++++---
 .../src/audio/audio_device_module_ios.h       |   4 +
 .../src/audio/audio_device_module_ios.mm      |   5 +
 .../src/audio/voice_processing_audio_unit.mm  | 106 ++++++++-----
 11 files changed, 306 insertions(+), 64 deletions(-)
 create mode 100644 sdk/objc/api/peerconnection/RTCAudioDeviceModule+Private.h
 create mode 100644 sdk/objc/api/peerconnection/RTCAudioDeviceModule.h
 create mode 100644 sdk/objc/api/peerconnection/RTCAudioDeviceModule.mm

diff --git a/sdk/BUILD.gn b/sdk/BUILD.gn
index 759682f1d4..7bc06d5b6c 100644
--- a/sdk/BUILD.gn
+++ b/sdk/BUILD.gn
@@ -936,6 +936,7 @@ if (is_ios || is_mac) {
       ]
       configs += [ "..:no_global_constructors" ]
       sources = [
+        "objc/api/peerconnection/RTCAudioDeviceModule.h",
         "objc/api/peerconnection/RTCAudioSource+Private.h",
         "objc/api/peerconnection/RTCAudioSource.h",
         "objc/api/peerconnection/RTCAudioSource.mm",
@@ -1103,6 +1104,10 @@ if (is_ios || is_mac) {
       ]
 
       if (is_ios) {
+        sources += [
+          "objc/api/peerconnection/RTCAudioDeviceModule+Private.h",
+          "objc/api/peerconnection/RTCAudioDeviceModule.mm",
+        ]
         deps += [ ":native_api_audio_device_module" ]
       }
     }
@@ -1306,6 +1311,7 @@ if (is_ios || is_mac) {
           "objc/helpers/RTCCameraPreviewView.h",
           "objc/helpers/RTCDispatcher.h",
           "objc/helpers/UIDevice+RTCDevice.h",
+          "objc/api/peerconnection/RTCAudioDeviceModule.h",
           "objc/api/peerconnection/RTCAudioSource.h",
           "objc/api/peerconnection/RTCAudioTrack.h",
           "objc/api/peerconnection/RTCConfiguration.h",
@@ -1418,6 +1424,7 @@ if (is_ios || is_mac) {
         output_name = "WebRTC"
 
         sources = [
+          "objc/api/peerconnection/RTCAudioDeviceModule.h",
           "objc/api/peerconnection/RTCAudioSource.h",
           "objc/api/peerconnection/RTCAudioTrack.h",
           "objc/api/peerconnection/RTCCertificate.h",
diff --git a/sdk/objc/api/peerconnection/RTCAudioDeviceModule+Private.h b/sdk/objc/api/peerconnection/RTCAudioDeviceModule+Private.h
new file mode 100644
index 0000000000..72ccd693c6
--- /dev/null
+++ b/sdk/objc/api/peerconnection/RTCAudioDeviceModule+Private.h
@@ -0,0 +1,16 @@
+#import "RTCAudioDeviceModule.h"
+
+#if defined(WEBRTC_IOS)
+#include "sdk/objc/native/src/audio/audio_device_module_ios.h"
+
+NS_ASSUME_NONNULL_BEGIN
+
+@interface RTCAudioDeviceModule ()
+
+@property(nonatomic, readonly) rtc::scoped_refptr<webrtc::ios_adm::AudioDeviceModuleIOS>
+    nativeModule;
+
+@end
+
+NS_ASSUME_NONNULL_END
+#endif
\ No newline at end of file
diff --git a/sdk/objc/api/peerconnection/RTCAudioDeviceModule.h b/sdk/objc/api/peerconnection/RTCAudioDeviceModule.h
new file mode 100644
index 0000000000..d8aba54508
--- /dev/null
+++ b/sdk/objc/api/peerconnection/RTCAudioDeviceModule.h
@@ -0,0 +1,18 @@
+
+#import <CoreMedia/CoreMedia.h>
+#import <Foundation/Foundation.h>
+
+#import "RTCMacros.h"
+
+NS_ASSUME_NONNULL_BEGIN
+
+RTC_OBJC_EXPORT
+
+NS_CLASS_AVAILABLE_IOS(2_0)
+@interface RTCAudioDeviceModule : NSObject
+
+- (void)deliverRecordedData:(CMSampleBufferRef)sampleBuffer;
+
+@end
+
+NS_ASSUME_NONNULL_END
\ No newline at end of file
diff --git a/sdk/objc/api/peerconnection/RTCAudioDeviceModule.mm b/sdk/objc/api/peerconnection/RTCAudioDeviceModule.mm
new file mode 100644
index 0000000000..531bac8eda
--- /dev/null
+++ b/sdk/objc/api/peerconnection/RTCAudioDeviceModule.mm
@@ -0,0 +1,27 @@
+
+#include <AudioUnit/AudioUnit.h>
+
+#import "RTCAudioDeviceModule+Private.h"
+#include "rtc_base/ref_counted_object.h"
+
+@implementation RTCAudioDeviceModule {
+  rtc::scoped_refptr<webrtc::ios_adm::AudioDeviceModuleIOS> _nativeModule;
+}
+
+- (instancetype)init {
+  self = [super init];
+  _nativeModule = new rtc::RefCountedObject<webrtc::ios_adm::AudioDeviceModuleIOS>(false);
+  return self;
+}
+
+- (void)deliverRecordedData:(CMSampleBufferRef)sampleBuffer {
+  _nativeModule->OnDeliverRecordedExternalData(sampleBuffer);
+}
+
+#pragma mark - Private
+
+- (rtc::scoped_refptr<webrtc::ios_adm::AudioDeviceModuleIOS>)nativeModule {
+  return _nativeModule;
+}
+
+@end
\ No newline at end of file
diff --git a/sdk/objc/api/peerconnection/RTCPeerConnectionFactory.h b/sdk/objc/api/peerconnection/RTCPeerConnectionFactory.h
index 5575af98c9..f48c14faed 100644
--- a/sdk/objc/api/peerconnection/RTCPeerConnectionFactory.h
+++ b/sdk/objc/api/peerconnection/RTCPeerConnectionFactory.h
@@ -11,6 +11,7 @@
 #import <Foundation/Foundation.h>
 
 #import "RTCMacros.h"
+#import "RTCAudioDeviceModule.h"
 
 NS_ASSUME_NONNULL_BEGIN
 
@@ -51,6 +52,10 @@ RTC_OBJC_EXPORT
             decoderFactory:(nullable id<RTC_OBJC_TYPE(RTCVideoDecoderFactory)>)decoderFactory
                audioDevice:(nullable id<RTC_OBJC_TYPE(RTCAudioDevice)>)audioDevice;
 
+- (instancetype)initWithEncoderFactory:(nullable id<RTCVideoEncoderFactory>)encoderFactory
+                                    decoderFactory:(nullable id<RTCVideoDecoderFactory>)decoderFactory
+                                 audioDeviceModule:(RTCAudioDeviceModule *)audioDeviceModule NS_AVAILABLE_IOS(2_0);
+
 /** Initialize an RTCAudioSource with constraints. */
 - (RTC_OBJC_TYPE(RTCAudioSource) *)audioSourceWithConstraints:
     (nullable RTC_OBJC_TYPE(RTCMediaConstraints) *)constraints;
diff --git a/sdk/objc/api/peerconnection/RTCPeerConnectionFactory.mm b/sdk/objc/api/peerconnection/RTCPeerConnectionFactory.mm
index c4d89e911d..6699facc04 100644
--- a/sdk/objc/api/peerconnection/RTCPeerConnectionFactory.mm
+++ b/sdk/objc/api/peerconnection/RTCPeerConnectionFactory.mm
@@ -14,6 +14,7 @@
 #import "RTCPeerConnectionFactory+Private.h"
 #import "RTCPeerConnectionFactoryOptions+Private.h"
 
+#import "RTCAudioDeviceModule+Private.h"
 #import "RTCAudioSource+Private.h"
 #import "RTCAudioTrack+Private.h"
 #import "RTCMediaConstraints+Private.h"
@@ -116,6 +117,32 @@ - (instancetype)init {
 #endif
 }
 
+#if defined(WEBRTC_IOS)
+- (instancetype)initWithEncoderFactory:(nullable id<RTCVideoEncoderFactory>)encoderFactory
+                        decoderFactory:(nullable id<RTCVideoDecoderFactory>)decoderFactory
+                     audioDeviceModule:(RTCAudioDeviceModule *)audioDeviceModule {
+	#ifdef HAVE_NO_MEDIA
+	  return [self initWithNoMedia];
+	#else
+	  std::unique_ptr<webrtc::VideoEncoderFactory> native_encoder_factory;
+	  std::unique_ptr<webrtc::VideoDecoderFactory> native_decoder_factory;
+	  if (encoderFactory) {
+		native_encoder_factory = webrtc::ObjCToNativeVideoEncoderFactory(encoderFactory);
+	  }
+	  if (decoderFactory) {
+		native_decoder_factory = webrtc::ObjCToNativeVideoDecoderFactory(decoderFactory);
+	  }
+	  return [self initWithNativeAudioEncoderFactory:webrtc::CreateBuiltinAudioEncoderFactory()
+						   nativeAudioDecoderFactory:webrtc::CreateBuiltinAudioDecoderFactory()
+						   nativeVideoEncoderFactory:std::move(native_encoder_factory)
+						   nativeVideoDecoderFactory:std::move(native_decoder_factory)
+								audioDeviceModule:audioDeviceModule.nativeModule.get()
+							   audioProcessingModule:nullptr];
+	#endif
+}
+#endif            
+              
+
 - (instancetype)initNative {
   if (self = [super init]) {
     _networkThread = rtc::Thread::CreateWithSocketServer();
diff --git a/sdk/objc/native/src/audio/audio_device_ios.h b/sdk/objc/native/src/audio/audio_device_ios.h
index a86acb56fe..eadd9f87e3 100644
--- a/sdk/objc/native/src/audio/audio_device_ios.h
+++ b/sdk/objc/native/src/audio/audio_device_ios.h
@@ -12,6 +12,7 @@
 #define SDK_OBJC_NATIVE_SRC_AUDIO_AUDIO_DEVICE_IOS_H_
 
 #include <atomic>
+#include <CoreMedia/CoreMedia.h>
 #include <memory>
 
 #include "api/scoped_refptr.h"
@@ -148,6 +149,8 @@ class AudioDeviceIOS : public AudioDeviceGeneric,
   void OnCanPlayOrRecordChange(bool can_play_or_record) override;
   void OnChangedOutputVolume() override;
 
+  void OnDeliverRecordedExternalData(CMSampleBufferRef sample_buffer);
+
   // VoiceProcessingAudioUnitObserver methods.
   OSStatus OnDeliverRecordedData(AudioUnitRenderActionFlags* flags,
                                  const AudioTimeStamp* time_stamp,
@@ -212,7 +215,10 @@ class AudioDeviceIOS : public AudioDeviceGeneric,
   const bool bypass_voice_processing_;
 
   // Native I/O audio thread checker.
-  SequenceChecker io_thread_checker_;
+  //SequenceChecker io_thread_checker_;
+
+   // Native audio I/O mutex.
+  Mutex io_mutex_;
 
   // Thread that this object is created on.
   rtc::Thread* thread_;
@@ -285,7 +291,7 @@ class AudioDeviceIOS : public AudioDeviceGeneric,
 
   // Counts number of detected audio glitches on the playout side.
   int64_t num_detected_playout_glitches_ RTC_GUARDED_BY(thread_);
-  int64_t last_playout_time_ RTC_GUARDED_BY(io_thread_checker_);
+  int64_t last_playout_time_ RTC_GUARDED_BY(io_mutex_);
 
   // Counts number of playout callbacks per call.
   // The value is updated on the native I/O thread and later read on the
diff --git a/sdk/objc/native/src/audio/audio_device_ios.mm b/sdk/objc/native/src/audio/audio_device_ios.mm
index dd2c11bdd2..5dfac4e438 100644
--- a/sdk/objc/native/src/audio/audio_device_ios.mm
+++ b/sdk/objc/native/src/audio/audio_device_ios.mm
@@ -10,7 +10,7 @@
 
 #import <AVFoundation/AVFoundation.h>
 #import <Foundation/Foundation.h>
-
+#import <CoreMedia/CoreMedia.h>
 #include "audio_device_ios.h"
 
 #include <cmath>
@@ -106,7 +106,7 @@ static void LogDeviceInfo() {
       last_output_volume_change_time_(0) {
   LOGI() << "ctor" << ios::GetCurrentThreadDescription()
          << ",bypass_voice_processing=" << bypass_voice_processing_;
-  io_thread_checker_.Detach();
+  //io_thread_checker_.Detach();
   thread_ = rtc::Thread::Current();
 
   audio_session_observer_ = [[RTCNativeAudioSessionDelegateAdapter alloc] initWithObserver:this];
@@ -129,7 +129,7 @@ static void LogDeviceInfo() {
 
 AudioDeviceGeneric::InitStatus AudioDeviceIOS::Init() {
   LOGI() << "Init";
-  io_thread_checker_.Detach();
+  //io_thread_checker_.Detach();
 
   RTC_DCHECK_RUN_ON(thread_);
   if (initialized_) {
@@ -360,12 +360,73 @@ static void LogDeviceInfo() {
   thread_->PostTask(SafeTask(safety_, [this] { HandleOutputVolumeChange(); }));
 }
 
+void AudioDeviceIOS::OnDeliverRecordedExternalData(CMSampleBufferRef sample_buffer) {
+  MutexLock scoped_lock(&io_mutex_);
+
+  if (audio_unit_ && audio_unit_->GetState() != VoiceProcessingAudioUnit::kUninitialized) {
+    RTCLogError(@"External recorded data was provided while audio unit is enabled.");
+    return;
+  }
+
+  CMFormatDescriptionRef description = CMSampleBufferGetFormatDescription(sample_buffer);
+  const AudioStreamBasicDescription *asbd = CMAudioFormatDescriptionGetStreamBasicDescription(description);
+  if (!asbd) {
+    RTCLogError(@"External recorded data was not in audio format.");
+    return;
+  }
+
+  if (asbd->mSampleRate != record_parameters_.sample_rate() ||
+      asbd->mChannelsPerFrame != record_parameters_.channels()) {
+    record_parameters_.reset(asbd->mSampleRate, asbd->mChannelsPerFrame);
+    UpdateAudioDeviceBuffer();
+
+    // Create a modified audio buffer class which allows us to ask for,
+    // or deliver, any number of samples (and not only multiple of 10ms) to match
+    // the native audio unit buffer size.
+    RTC_DCHECK(audio_device_buffer_);
+    fine_audio_buffer_.reset(new FineAudioBuffer(audio_device_buffer_));
+  }
+
+  CMBlockBufferRef block_buffer = CMSampleBufferGetDataBuffer(sample_buffer);
+  if (block_buffer == nil) {
+    return;
+  }
+
+  AudioBufferList buffer_list;
+  CMSampleBufferGetAudioBufferListWithRetainedBlockBuffer(sample_buffer,
+                                                          nullptr,
+                                                          &buffer_list,
+                                                          sizeof(buffer_list),
+                                                          nullptr,
+                                                          nullptr,
+                                                          kCMSampleBufferFlag_AudioBufferList_Assure16ByteAlignment,
+                                                          &block_buffer);
+
+  rtc::ArrayView<int16_t> view {
+    static_cast<int16_t*>(buffer_list.mBuffers[0].mData),
+    buffer_list.mBuffers[0].mDataByteSize / sizeof(int16_t)
+  };
+
+  if (asbd->mFormatFlags & kAudioFormatFlagIsBigEndian) {
+    for (auto& element : view) {
+      element = be16toh(element);
+    }
+  }
+
+  fine_audio_buffer_->DeliverRecordedData(view, kFixedRecordDelayEstimate);
+
+  CFRelease(block_buffer);
+}
+
+
+
 OSStatus AudioDeviceIOS::OnDeliverRecordedData(AudioUnitRenderActionFlags* flags,
                                                const AudioTimeStamp* time_stamp,
                                                UInt32 bus_number,
                                                UInt32 num_frames,
                                                AudioBufferList* /* io_data */) {
-  RTC_DCHECK_RUN_ON(&io_thread_checker_);
+  //RTC_DCHECK_RUN_ON(&io_thread_checker_);
+  MutexLock scoped_lock(&io_mutex_);
   OSStatus result = noErr;
   // Simply return if recording is not enabled.
   if (!recording_.load(std::memory_order_acquire)) return result;
@@ -413,8 +474,9 @@ static void LogDeviceInfo() {
                                           UInt32 bus_number,
                                           UInt32 num_frames,
                                           AudioBufferList* io_data) {
-  RTC_DCHECK_RUN_ON(&io_thread_checker_);
+  //RTC_DCHECK_RUN_ON(&io_thread_checker_);
   // Verify 16-bit, noninterleaved mono PCM signal format.
+  MutexLock scoped_lock(&io_mutex_);
   RTC_DCHECK_EQ(1, io_data->mNumberBuffers);
   AudioBuffer* audio_buffer = &io_data->mBuffers[0];
   RTC_DCHECK_EQ(1, audio_buffer->mNumberChannels);
@@ -638,15 +700,28 @@ static void LogDeviceInfo() {
   // AttachAudioBuffer() is called at construction by the main class but check
   // just in case.
   RTC_DCHECK(audio_device_buffer_) << "AttachAudioBuffer must be called first";
-  RTC_DCHECK_GT(playout_parameters_.sample_rate(), 0);
-  RTC_DCHECK_GT(record_parameters_.sample_rate(), 0);
-  RTC_DCHECK_EQ(playout_parameters_.channels(), 1);
-  RTC_DCHECK_EQ(record_parameters_.channels(), 1);
+  //RTC_DCHECK_GT(playout_parameters_.sample_rate(), 0);
+  //RTC_DCHECK_GT(record_parameters_.sample_rate(), 0);
+  //RTC_DCHECK_EQ(playout_parameters_.channels(), 1);
+  //RTC_DCHECK_EQ(record_parameters_.channels(), 1);
   // Inform the audio device buffer (ADB) about the new audio format.
-  audio_device_buffer_->SetPlayoutSampleRate(playout_parameters_.sample_rate());
-  audio_device_buffer_->SetPlayoutChannels(playout_parameters_.channels());
-  audio_device_buffer_->SetRecordingSampleRate(record_parameters_.sample_rate());
-  audio_device_buffer_->SetRecordingChannels(record_parameters_.channels());
+  //audio_device_buffer_->SetPlayoutSampleRate(playout_parameters_.sample_rate());
+  //audio_device_buffer_->SetPlayoutChannels(playout_parameters_.channels());
+  //audio_device_buffer_->SetRecordingSampleRate(record_parameters_.sample_rate());
+  //audio_device_buffer_->SetRecordingChannels(record_parameters_.channels());
+
+  if (playout_parameters_.is_valid()) {
+     //RTC_DCHECK_EQ(playout_parameters_.channels(), 1);
+     audio_device_buffer_->SetPlayoutSampleRate(playout_parameters_.sample_rate());
+     audio_device_buffer_->SetPlayoutChannels(playout_parameters_.channels());
+   }
+   if (record_parameters_.is_valid()) {
+     //RTC_DCHECK_EQ(record_parameters_.channels(), 1);
+     audio_device_buffer_->SetRecordingSampleRate(record_parameters_.sample_rate());
+     audio_device_buffer_->SetRecordingChannels(record_parameters_.channels());
+   }
+
+
 }
 
 void AudioDeviceIOS::SetupAudioBuffersForActiveAudioSession() {
@@ -683,9 +758,13 @@ static void LogDeviceInfo() {
   // number of audio frames.
   // Example: IO buffer size = 0.008 seconds <=> 128 audio frames at 16kHz.
   // Hence, 128 is the size we expect to see in upcoming render callbacks.
-  playout_parameters_.reset(sample_rate, playout_parameters_.channels(), io_buffer_duration);
+  //playout_parameters_.reset(sample_rate, playout_parameters_.channels(), io_buffer_duration);
+  playout_parameters_.reset(sample_rate, webRTCConfig.outputNumberOfChannels, io_buffer_duration);
+   
   RTC_DCHECK(playout_parameters_.is_complete());
-  record_parameters_.reset(sample_rate, record_parameters_.channels(), io_buffer_duration);
+  //record_parameters_.reset(sample_rate, record_parameters_.channels(), io_buffer_duration);
+  record_parameters_.reset(sample_rate, webRTCConfig.inputNumberOfChannels, io_buffer_duration);
+   
   RTC_DCHECK(record_parameters_.is_complete());
   RTC_LOG(LS_INFO) << " frames per I/O buffer: " << playout_parameters_.frames_per_buffer();
   RTC_LOG(LS_INFO) << " bytes per I/O buffer: " << playout_parameters_.GetBytesPerBuffer();
@@ -729,7 +808,12 @@ static void LogDeviceInfo() {
   if (!audio_is_initialized_) return;
 
   // If we're initialized, we must have an audio unit.
-  RTC_DCHECK(audio_unit_);
+  //RTC_DCHECK(audio_unit_);
+
+  if (can_play_or_record && !audio_unit_ && !CreateAudioUnit()) {
+    RTCLog(@"Failed to create audio unit.");
+    return;
+  }
 
   bool should_initialize_audio_unit = false;
   bool should_uninitialize_audio_unit = false;
@@ -859,9 +943,9 @@ static void LogDeviceInfo() {
   RTC_DCHECK_RUN_ON(thread_);
 
   // There should be no audio unit at this point.
-  if (!CreateAudioUnit()) {
-    return false;
-  }
+  //if (!CreateAudioUnit()) {
+  //  return false;
+  //}
 
   RTC_OBJC_TYPE(RTCAudioSession)* session = [RTC_OBJC_TYPE(RTCAudioSession) sharedInstance];
   // Subscribe to audio session events.
@@ -881,6 +965,25 @@ static void LogDeviceInfo() {
   // If we are ready to play or record, and if the audio session can be
   // configured, then initialize the audio unit.
   if (session.canPlayOrRecord) {
+      // Store the preferred sample rate and preferred number of channels already
+	    // here. They have not been set and confirmed yet since configureForWebRTC
+	    // is not called until audio is about to start. However, it makes sense to
+	    // store the parameters now and then verify at a later stage.
+	    RTC_OBJC_TYPE(RTCAudioSessionConfiguration)* config =
+	        [RTC_OBJC_TYPE(RTCAudioSessionConfiguration) webRTCConfiguration];
+	    playout_parameters_.reset(config.sampleRate, config.outputNumberOfChannels);
+	    record_parameters_.reset(config.sampleRate, config.inputNumberOfChannels);
+	    // Ensure that the audio device buffer (ADB) knows about the internal audio
+	    // parameters. Note that, even if we are unable to get a mono audio session,
+	    // we will always tell the I/O audio unit to do a channel format conversion
+	    // to guarantee mono on the "input side" of the audio unit.
+	    UpdateAudioDeviceBuffer();
+
+	    // There should be no audio unit at this point.
+	    if (!CreateAudioUnit()) {
+	      [session unlockForConfiguration];
+	      return false;
+	    }  
     if (!ConfigureAudioSessionLocked()) {
       // One possible reason for failure is if an attempt was made to use the
       // audio session during or after a Media Services failure.
@@ -910,7 +1013,7 @@ static void LogDeviceInfo() {
 
   // Detach thread checker for the AURemoteIO::IOThread to ensure that the
   // next session uses a fresh thread id.
-  io_thread_checker_.Detach();
+  //io_thread_checker_.Detach();
 
   // Remove audio session notification observers.
   RTC_OBJC_TYPE(RTCAudioSession)* session = [RTC_OBJC_TYPE(RTCAudioSession) sharedInstance];
@@ -927,7 +1030,7 @@ static void LogDeviceInfo() {
   // restart. It will result in audio callbacks from a new native I/O thread
   // which means that we must detach thread checkers here to be prepared for an
   // upcoming new audio stream.
-  io_thread_checker_.Detach();
+  //io_thread_checker_.Detach();
 }
 
 bool AudioDeviceIOS::IsInterrupted() {
diff --git a/sdk/objc/native/src/audio/audio_device_module_ios.h b/sdk/objc/native/src/audio/audio_device_module_ios.h
index 9bcf114e32..5946b0913c 100644
--- a/sdk/objc/native/src/audio/audio_device_module_ios.h
+++ b/sdk/objc/native/src/audio/audio_device_module_ios.h
@@ -19,6 +19,9 @@
 #include "modules/audio_device/audio_device_buffer.h"
 #include "modules/audio_device/include/audio_device.h"
 #include "rtc_base/checks.h"
+#if defined(WEBRTC_IOS)
+#include <CoreMedia/CoreMedia.h>
+#endif
 
 namespace webrtc {
 
@@ -127,6 +130,7 @@ class AudioDeviceModuleIOS : public AudioDeviceModule {
   int32_t GetPlayoutUnderrunCount() const override;
 
 #if defined(WEBRTC_IOS)
+  void OnDeliverRecordedExternalData(CMSampleBufferRef sample_buffer);
   int GetPlayoutAudioParameters(AudioParameters* params) const override;
   int GetRecordAudioParameters(AudioParameters* params) const override;
 #endif  // WEBRTC_IOS
diff --git a/sdk/objc/native/src/audio/audio_device_module_ios.mm b/sdk/objc/native/src/audio/audio_device_module_ios.mm
index 5effef3abd..1753e0820d 100644
--- a/sdk/objc/native/src/audio/audio_device_module_ios.mm
+++ b/sdk/objc/native/src/audio/audio_device_module_ios.mm
@@ -649,6 +649,11 @@
   }
 
 #if defined(WEBRTC_IOS)
+
+  void AudioDeviceModuleIOS::OnDeliverRecordedExternalData(CMSampleBufferRef sample_buffer) {
+    audio_device_->OnDeliverRecordedExternalData(sample_buffer);
+  }
+
   int AudioDeviceModuleIOS::GetPlayoutAudioParameters(
       AudioParameters* params) const {
     RTC_DLOG(LS_INFO) << __FUNCTION__;
diff --git a/sdk/objc/native/src/audio/voice_processing_audio_unit.mm b/sdk/objc/native/src/audio/voice_processing_audio_unit.mm
index 3905b6857a..98615eba37 100644
--- a/sdk/objc/native/src/audio/voice_processing_audio_unit.mm
+++ b/sdk/objc/native/src/audio/voice_processing_audio_unit.mm
@@ -112,16 +112,23 @@ static OSStatus GetAGCState(AudioUnit audio_unit, UInt32* enabled) {
   }
 
   // Enable input on the input scope of the input element.
-  UInt32 enable_input = 1;
-  result = AudioUnitSetProperty(vpio_unit_, kAudioOutputUnitProperty_EnableIO,
-                                kAudioUnitScope_Input, kInputBus, &enable_input,
-                                sizeof(enable_input));
-  if (result != noErr) {
-    DisposeAudioUnit();
-    RTCLogError(@"Failed to enable input on input scope of input element. "
-                 "Error=%ld.",
-                (long)result);
-    return false;
+  RTCAudioSessionConfiguration* webRTCConfiguration =  [RTCAudioSessionConfiguration webRTCConfiguration];
+  if (webRTCConfiguration.mode != AVAudioSessionModeMoviePlayback) 
+  {
+    UInt32 enable_input = 1;
+    result = AudioUnitSetProperty(vpio_unit_, kAudioOutputUnitProperty_EnableIO,
+                                  kAudioUnitScope_Input, kInputBus, &enable_input,
+                                  sizeof(enable_input));
+    if (result != noErr) {
+      DisposeAudioUnit();
+      RTCLogError(@"Failed to enable input on input scope of input element. "
+                  "Error=%ld.",
+                  (long)result);
+      return false;
+    }
+  }
+  else {
+     RTCLog("@Not Enable input on the input scope of the input element.");
   }
 
   // Enable output on the output scope of the output element.
@@ -155,34 +162,44 @@ static OSStatus GetAGCState(AudioUnit audio_unit, UInt32* enabled) {
 
   // Disable AU buffer allocation for the recorder, we allocate our own.
   // TODO(henrika): not sure that it actually saves resource to make this call.
-  UInt32 flag = 0;
-  result = AudioUnitSetProperty(
-      vpio_unit_, kAudioUnitProperty_ShouldAllocateBuffer,
-      kAudioUnitScope_Output, kInputBus, &flag, sizeof(flag));
-  if (result != noErr) {
-    DisposeAudioUnit();
-    RTCLogError(@"Failed to disable buffer allocation on the input bus. "
-                 "Error=%ld.",
-                (long)result);
-    return false;
+  if (webRTCConfiguration.mode != AVAudioSessionModeMoviePlayback) {
+    UInt32 flag = 0;
+    result = AudioUnitSetProperty(
+        vpio_unit_, kAudioUnitProperty_ShouldAllocateBuffer,
+        kAudioUnitScope_Output, kInputBus, &flag, sizeof(flag));
+    if (result != noErr) {
+      DisposeAudioUnit();
+      RTCLogError(@"Failed to disable buffer allocation on the input bus. "
+                  "Error=%ld.",
+                  (long)result);
+      return false;
+    }
+  }
+  else {
+     RTCLog("@Webrtc mode is movie and it's not allocating buffer for the recorder ");
   }
 
   // Specify the callback to be called by the I/O thread to us when input audio
   // is available. The recorded samples can then be obtained by calling the
   // AudioUnitRender() method.
-  AURenderCallbackStruct input_callback;
-  input_callback.inputProc = OnDeliverRecordedData;
-  input_callback.inputProcRefCon = this;
-  result = AudioUnitSetProperty(vpio_unit_,
-                                kAudioOutputUnitProperty_SetInputCallback,
-                                kAudioUnitScope_Global, kInputBus,
-                                &input_callback, sizeof(input_callback));
-  if (result != noErr) {
-    DisposeAudioUnit();
-    RTCLogError(@"Failed to specify the input callback on the input bus. "
-                 "Error=%ld.",
-                (long)result);
-    return false;
+  if (webRTCConfiguration.mode != AVAudioSessionModeMoviePlayback) {
+    AURenderCallbackStruct input_callback;
+    input_callback.inputProc = OnDeliverRecordedData;
+    input_callback.inputProcRefCon = this;
+    result = AudioUnitSetProperty(vpio_unit_,
+                                  kAudioOutputUnitProperty_SetInputCallback,
+                                  kAudioUnitScope_Global, kInputBus,
+                                  &input_callback, sizeof(input_callback));
+    if (result != noErr) {
+      DisposeAudioUnit();
+      RTCLogError(@"Failed to specify the input callback on the input bus. "
+                  "Error=%ld.",
+                  (long)result);
+      return false;
+    }
+  }
+  else {
+     RTCLog("@WebRTC mode is movie and it's not arranging the callback");
   }
 
   state_ = kUninitialized;
@@ -205,14 +222,21 @@ static OSStatus GetAGCState(AudioUnit audio_unit, UInt32* enabled) {
 #endif
 
   // Set the format on the output scope of the input element/bus.
-  result =
-      AudioUnitSetProperty(vpio_unit_, kAudioUnitProperty_StreamFormat,
-                           kAudioUnitScope_Output, kInputBus, &format, size);
-  if (result != noErr) {
-    RTCLogError(@"Failed to set format on output scope of input bus. "
-                 "Error=%ld.",
-                (long)result);
-    return false;
+  RTCAudioSessionConfiguration* webRTCConfiguration =  [RTCAudioSessionConfiguration webRTCConfiguration];
+  if (webRTCConfiguration.mode != AVAudioSessionModeMoviePlayback) 
+  {
+    result =
+        AudioUnitSetProperty(vpio_unit_, kAudioUnitProperty_StreamFormat,
+                            kAudioUnitScope_Output, kInputBus, &format, size);
+    if (result != noErr) {
+      RTCLogError(@"Failed to set format on output scope of input bus. "
+                  "Error=%ld.",
+                  (long)result);
+      return false;
+    }
+  }
+  else {
+    RTCLog("@NOT setting the format on the output sscope of the input element because it's movie mode");
   }
 
   // Set the format on the input scope of the output element/bus.
-- 
2.37.1 (Apple Git-137.1)

